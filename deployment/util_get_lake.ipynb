{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Get Lake\n",
    "\n",
    "| Parameter | Description | Use Case |\n",
    "|---------- |-------------|----------|\n",
    "| name | Name of the lakehouse to get/create | Always |\n",
    "| ws_id | Target location workspace ID | Always |\n",
    "| desc | Lakehouse description if deploying. | When deploying |\n",
    "| schemas | Default is True | When deploying |\n",
    "\n",
    "\n",
    "Purpose:\n",
    "1. Try to get a lakehouse artifact\n",
    "2. Deploy one if not found\n",
    "3. Try again(in case it was a transient error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# --- PARAMETERS--- #\n",
    "name: str = \"\"\n",
    "ws_id: str = \"\"\n",
    "desc: str = \"\"\n",
    "schemas: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# --- LAKEHOUSE GET/DEPLOY FUNCTION --- #\n",
    "def get_or_create_lakehouse(name: str, *, workspace_id: str, description: str = \"\", enable_schemas: bool = True):\n",
    "    try: \n",
    "        return notebookutils.lakehouse.get(name, workspace_id) # -> get lh\n",
    "    \n",
    "    except Exception: \n",
    "        definition = {\"enableSchemas\": True} if enable_schemas else None # -> with schemas enabled\n",
    "        \n",
    "        try:\n",
    "            return notebookutils.lakehouse.create(name, description, definition, workspace_id) # -> create with definition\n",
    "        \n",
    "        except Exception: # -> in case was transient\n",
    "            return notebookutils.lakehouse.get(name, workspace_id) # -> get lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# --- EXECUTION --- #\n",
    "lake = get_or_create_lakehouse(\n",
    "    name=name,\n",
    "    workspace_id=ws_id,\n",
    "    description=desc,\n",
    "    enable_schemas=schemas\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# --- EXIT --- #\n",
    "import json\n",
    "notebookutils.notebook.exit(json.dumps(lake))"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
