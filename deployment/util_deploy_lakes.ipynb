{"cells":[{"cell_type":"markdown","source":["# Idempotent Lakehouse Deployment\n","\n","**Notebook Purpose**\n","\n","This notebook orchestrates idempotent deployment and validation of core lakehouse layers (Ops, Bronze, Silver, Gold) inside the current Microsoft Fabric workspace. It retrieves workspace context at runtime, constructs a parallelized execution DAG, and calls a reusable utility notebook to get-or-create each lakehouse artifact with standardized configuration (schemas enabled, descriptions applied). The result is a consistent, automated foundation for downstream ingestion, transformation, and reporting pipelines."],"metadata":{},"id":"bccb1eb1-ac4b-4cd2-90ba-71f3275245f4"},{"cell_type":"code","source":["# --- FETCH CONTEXT VARIABLES --- #\n","ctx = notebookutils.runtime.context # -> get context\n","\n","# Extract the current workspace identifiers from the runtime context.\n","# These are used to ensure lakehouse artifacts are created/looked up in the correct workspace.\n","WORKSPACE_ID, WORKSPACE_NAME = (\n","    ctx.get(\"currentWorkspaceId\"),\n","    ctx.get(\"currentWorkspaceName\")\n",")\n","\n","# Defensive check: context lookups can fail if the notebook is running in an unexpected\n","# execution environment (e.g., missing permissions, detached context, or non-standard runtime).\n","# Fail fast so downstream lakehouse deployment doesn't accidentally target an unknown workspace.\n","if not WORKSPACE_ID or not WORKSPACE_NAME:\n","    raise ValueError(\"Could not determine workspace from context\")\n","\n","\n","# --- LAKE ARTIFACTS --- #\n","# Define the lakehouses you want to ensure exist (idempotent deploy / get-or-create pattern),\n","# along with human-readable descriptions to apply at creation time.\n","lakes = {\n","    \"Ops\": \"Operations / Control Layer\",\n","    \"Bronze\": \"Data Landing Zone / Raw Data\",\n","    \"Silver\": \"Curated Base Data / Pre Business Logic\",\n","    \"Gold\": \"Reporting Layer\"\n","}\n","\n","# Build a list of \"activities\" (one activity per lakehouse).\n","# Each activity calls a child utility notebook (\"util_get_lake\") that is responsible for:\n","#   1) trying to retrieve the lakehouse artifact by name in the given workspace, and\n","#   2) creating it if it does not exist.\n","activities = [\n","    {\n","        \"name\": lh_name,\n","        \"path\": \"util_get_lake\",\n","        \"timeoutPerCellInSeconds\": 120,\n","        \"args\": {\n","            \"name\": lh_name,                  # Lakehouse name to get-or-create\n","            \"ws_id\": WORKSPACE_ID,            # Ensure the operation targets the current workspace\n","            \"desc\": desc,                     # Description used on creation\n","            \"schemas\": True,                  # Enable schemas on the lakehouse definition\n","        },\n","        \"retry\": 0,\n","        \"retryIntervalInSeconds\": 10\n","    }\n","    for lh_name, desc in lakes.items()\n","]\n","\n","# Define the Directed Acyclic Graph (DAG) configuration for runMultiple.\n","DAG = {\n","    \"activities\": activities,\n","    \"concurrency\": min(4, len(activities)),  # Adjust if your fabric sku is a limiting factor; avoid over-parallelizing;\n","    \"timeoutInSeconds\": 240,                 # Total time allowed for the whole DAG\n","}\n","\n","# --- EXECUTE LAKE DEPLOYMENT --- #\n","# Execute all activities according to the DAG definition.\n","results = notebookutils.notebook.runMultiple(\n","    DAG,\n","    {\"displayDAGViaGraphviz\": False}         # displayDAGViaGraphviz=False disables graph visualization output.\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"312c2917-3376-4dd9-8e38-09262d8b1bfb"},{"cell_type":"code","source":["# --- PARSE DAG RESULTS --- #\n","import json\n","\n","def parse_runmultiple(results: dict) -> dict:\n","    # 1) collect failures\n","    failures = {\n","        name: info[\"exception\"]\n","        for name, info in results.items()\n","        if info.get(\"exception\") is not None\n","    }\n","    if failures:\n","        msg = \"\\n\".join(f\"- {name}: {err}\" for name, err in failures.items())\n","        raise RuntimeError(f\"One or more lakehouse deployments failed:\\n{msg}\")\n","\n","    # 2) decode exitVal JSON\n","    return {\n","        name: json.loads(info[\"exitVal\"])\n","        for name, info in results.items()\n","    }\n","\n","\n","artifacts = parse_runmultiple(results)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cdf2cd87-c777-464e-b44b-06a72dbc2d45"}],"metadata":{"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"kernel_info":{"name":"synapse_pyspark"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}}},"nbformat":4,"nbformat_minor":5}