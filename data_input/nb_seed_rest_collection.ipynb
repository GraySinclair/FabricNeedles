{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "###### **Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Input values here\n",
    "source_system: str = \"\"\n",
    "table_name: str = \"\"\n",
    "endpoint: str = \"\"\n",
    "http_method: str = \"\"\n",
    "watermark_column: str = \"\"\n",
    "max_watermark_cutoff: str = \"\" # fmt: '2023-12-01T00:00:00Z'\n",
    "page_size: int = 500\n",
    "query_template: str = \"\"\n",
    "headers_json: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "###### **Dynamic Path Resolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "ctx = notebookutils.runtime.context\n",
    "wsid = ctx.get(\"currentWorkspaceId\")\n",
    "\n",
    "if not wsid: raise ValueError(\"Missing `currentWorkspaceId` from notebook runtime context.\")\n",
    "\n",
    "try:\n",
    "    ops_artifact = notebookutils.lakehouse.get(name=\"Ops\", workspaceId=wsid)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Could not return `Ops` lakehouse in this workspace ({wsid=}). Error: {repr(e)}\") from e\n",
    "\n",
    "if ops_artifact is None: raise ValueError(\"Ops artifact is None\")\n",
    "\n",
    "props = ops_artifact.get(\"properties\") or {}\n",
    "abfs = props.get(\"abfsPath\")\n",
    "\n",
    "if not abfs: raise ValueError(\"Ops artifact missing `properties.abfsPath`\")\n",
    "\n",
    "rest_collection_path = f\"{abfs}/Tables/ops/rest_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame, functions as F, types as T\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def _rest_collection_schema() -> T.StructType:\n",
    "    return T.StructType(\n",
    "        [\n",
    "            T.StructField(\"source_system\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"table_name\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"endpoint\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"http_method\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"watermark_column\", T.StringType(), nullable=True),\n",
    "            T.StructField(\"max_watermark_cutoff\", T.TimestampType(), nullable=True),\n",
    "            T.StructField(\"page_size\", T.IntegerType(), nullable=True),\n",
    "            T.StructField(\"query_template\", T.StringType(), nullable=True),\n",
    "            T.StructField(\"headers_json\", T.StringType(), nullable=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# convert string format to timestamp\n",
    "max_watermark_cutoff = datetime.strptime(max_watermark_cutoff, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "# Single-row source DF (the new/updated config row)\n",
    "src_df = spark.createDataFrame([{\n",
    "    \"source_system\": source_system,\n",
    "    \"table_name\": table_name,\n",
    "    \"endpoint\": endpoint,\n",
    "    \"http_method\": http_method,\n",
    "    \"watermark_column\": watermark_column,\n",
    "    \"max_watermark_cutoff\": max_watermark_cutoff,\n",
    "    \"page_size\": page_size,\n",
    "    \"query_template\": query_template,\n",
    "    \"headers_json\": headers_json,\n",
    "}], schema=_rest_collection_schema())\n",
    "\n",
    "\n",
    "display(src_df) # preview row to be added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "###### **Merge Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Target delta table at ABFSS path\n",
    "tgt = DeltaTable.forPath(spark, rest_collection_path)\n",
    "\n",
    "(\n",
    "    tgt.alias(\"t\")\n",
    "       .merge(\n",
    "            src_df.alias(\"s\"),\n",
    "            \"t.source_system = s.source_system AND t.table_name = s.table_name\"\n",
    "        )\n",
    "       .whenMatchedUpdate(set={\n",
    "            \"endpoint\": \"s.endpoint\",\n",
    "            \"http_method\": \"s.http_method\",\n",
    "            \"watermark_column\": \"s.watermark_column\",\n",
    "            \"max_watermark_cutoff\": \"s.max_watermark_cutoff\",\n",
    "            \"page_size\": \"s.page_size\",\n",
    "            \"query_template\": \"s.query_template\",\n",
    "            \"headers_json\": \"s.headers_json\",\n",
    "        })\n",
    "       .whenNotMatchedInsertAll()\n",
    "       .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "###### **Preview Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "preview = True\n",
    "\n",
    "if not preview: notebookutils.notebook.exit(\"\")\n",
    "\n",
    "df = spark.read.format(\"delta\").load(rest_collection_path)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
