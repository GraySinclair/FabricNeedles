{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "###### **Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "source_layer: str = \"\"            # use the destination layer e.g. 'Bronze', 'Silver', 'Gold'(use the lakehouse names)\n",
    "source_system: str = \"\"           # wherever the data came from e.g. 'lcvista', 'intacct'(will be used for the schema)\n",
    "source_table_name: str = \"\"       # e.g. 'staff'\n",
    "target_table_name: str = \"\"       # usually the same as source_table_name unless splitting into multiple tables\n",
    "primary_keys_json: str = \"\"       # single key: 'staff_id' | composite keys: 'staff_id,department_id'(comma-delim & single-string) ->split string on `,` in logic\n",
    "watermark_column: str = \"\"        # e.g. 'modified'\n",
    "max_watermark_cutoff: str = \"\"    # fmt '2023-12-01T00:00:00Z'\n",
    "schema_json: str = \"\"             # use df.schema.json() and store here\n",
    "transform_mode: str = \"\"\n",
    "transform_notebook: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "###### **Dynamic Path Resolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "ctx = notebookutils.runtime.context\n",
    "wsid = ctx.get(\"currentWorkspaceId\")\n",
    "\n",
    "if not wsid: raise ValueError(\"Missing `currentWorkspaceId` from notebook runtime context.\")\n",
    "\n",
    "try:\n",
    "    ops_artifact = notebookutils.lakehouse.get(name=\"Ops\", workspaceId=wsid)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Could not return `Ops` lakehouse in this workspace ({wsid=}). Error: {repr(e)}\") from e\n",
    "\n",
    "if ops_artifact is None: raise ValueError(\"Ops artifact is None\")\n",
    "\n",
    "props = ops_artifact.get(\"properties\") or {}\n",
    "abfs = props.get(\"abfsPath\")\n",
    "\n",
    "if not abfs: raise ValueError(\"Ops artifact missing `properties.abfsPath`\")\n",
    "\n",
    "specs_table_path = f\"{abfs}/Tables/ops/specs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import types as T\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "def _specs_schema() -> T.StructType:\n",
    "    return T.StructType(\n",
    "        [\n",
    "            T.StructField(\"source_layer\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"source_system\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"source_table_name\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"target_table_name\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"primary_keys_json\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"watermark_column\", T.StringType(), nullable=True),\n",
    "            T.StructField(\"max_watermark_cutoff\", T.TimestampType(), nullable=True),\n",
    "            T.StructField(\"schema_json\", T.StringType(), nullable=True),\n",
    "            T.StructField(\"transform_mode\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"transform_notebook\", T.StringType(), nullable=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# convert string format to timestamp\n",
    "max_watermark_cutoff = datetime.strptime(max_watermark_cutoff, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "\n",
    "# Single-row source DF (the new/updated spec row)\n",
    "src_df = spark.createDataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"source_layer\": source_layer,\n",
    "            \"source_system\": source_system,\n",
    "            \"source_table_name\": source_table_name,\n",
    "            \"target_table_name\": target_table_name,\n",
    "            \"primary_keys_json\": primary_keys_json,\n",
    "            \"watermark_column\": watermark_column,\n",
    "            \"max_watermark_cutoff\": max_watermark_cutoff,\n",
    "            \"schema_json\": schema_json,\n",
    "            \"transform_mode\": transform_mode,\n",
    "            \"transform_notebook\": transform_notebook,\n",
    "        }\n",
    "    ],\n",
    "    schema=_specs_schema()\n",
    ")\n",
    "\n",
    "display(src_df)  # preview row to be merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "###### **Merge Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Target delta table at ABFSS path\n",
    "tgt = DeltaTable.forPath(spark, specs_table_path)\n",
    "\n",
    "merge_cond = (\n",
    "    \"t.source_layer = s.source_layer \"\n",
    "    \"AND t.source_system = s.source_system \"\n",
    "    \"AND t.source_table_name = s.source_table_name\"\n",
    ")\n",
    "\n",
    "(\n",
    "    tgt.alias(\"t\")\n",
    "       .merge(src_df.alias(\"s\"), merge_cond)\n",
    "       .whenMatchedUpdate(set={\n",
    "            \"target_table_name\": \"s.target_table_name\",\n",
    "            \"primary_keys_json\": \"s.primary_keys_json\",\n",
    "            \"watermark_column\": \"s.watermark_column\",\n",
    "            \"max_watermark_cutoff\": \"s.max_watermark_cutoff\",\n",
    "            \"schema_json\": \"s.schema_json\",\n",
    "            \"transform_mode\": \"s.transform_mode\",\n",
    "            \"transform_notebook\": \"s.transform_notebook\",\n",
    "            \"_updated_ts\": \"current_timestamp()\",\n",
    "        })\n",
    "       .whenNotMatchedInsert(values={\n",
    "            \"source_layer\": \"s.source_layer\",\n",
    "            \"source_system\": \"s.source_system\",\n",
    "            \"source_table_name\": \"s.source_table_name\",\n",
    "            \"target_table_name\": \"s.target_table_name\",\n",
    "            \"primary_keys_json\": \"s.primary_keys_json\",\n",
    "            \"watermark_column\": \"s.watermark_column\",\n",
    "            \"max_watermark_cutoff\": \"s.max_watermark_cutoff\",\n",
    "            \"schema_json\": \"s.schema_json\",\n",
    "            \"transform_mode\": \"s.transform_mode\",\n",
    "            \"transform_notebook\": \"s.transform_notebook\",\n",
    "            \"_created_ts\": \"current_timestamp()\",\n",
    "            \"_updated_ts\": \"current_timestamp()\",\n",
    "        })\n",
    "       .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "###### **Preview Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "preview = True\n",
    "\n",
    "if not preview: notebookutils.notebook.exit(\"\")\n",
    "\n",
    "df = spark.read.format(\"delta\").load(specs_table_path)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#TODO add guardrails around certain inputs"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
