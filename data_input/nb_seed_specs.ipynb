{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "###### **Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Input values here\n",
    "source_layer: str = \"\"            # e.g. 'bronze', 'silver', 'gold'\n",
    "source_system: str = \"\"           # e.g. 'lcvista', 'intacct'\n",
    "source_table_name: str = \"\"       # logical name, e.g. 'staff'\n",
    "target_table_name: str = \"\"       # e.g. 'lcvista.staff'\n",
    "primary_keys_json: str = \"\"       # JSON array string, e.g. '[\"id\"]'\n",
    "watermark_column: str = \"\"        # optional, e.g. 'modified'\n",
    "max_watermark_cuttoff: str = \"\"   # optional ISO UTC, e.g. '2023-12-01T00:00:00Z'\n",
    "schema_json: str = \"\"             # optional StructType JSON string\n",
    "transform_mode: str = \"PASS\"      # PASS | FLATTEN | CUSTOM\n",
    "transform_notebook: str = \"\"      # optional if transform_mode='CUSTOM'\n",
    "\n",
    "# Set to False to skip the final display\n",
    "preview: bool = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "###### **Dynamic Path Resolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "ctx = notebookutils.runtime.context\n",
    "wsid = ctx.get(\"currentWorkspaceId\")\n",
    "\n",
    "if not wsid:\n",
    "    raise ValueError(\"Missing `currentWorkspaceId` from notebook runtime context.\")\n",
    "\n",
    "try:\n",
    "    ops_artifact = notebookutils.lakehouse.get(name=\"Ops\", workspaceId=wsid)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        f\"Could not resolve `Ops` lakehouse in this workspace ({wsid=}). Error: {repr(e)}\"\n",
    "    ) from e\n",
    "\n",
    "if ops_artifact is None:\n",
    "    raise ValueError(\"Ops artifact is None\")\n",
    "\n",
    "props = ops_artifact.get(\"properties\") or {}\n",
    "abfs = props.get(\"abfsPath\")\n",
    "if not abfs:\n",
    "    raise ValueError(\"Ops artifact missing `properties.abfsPath`\")\n",
    "\n",
    "# Delta table path (schema-enabled lakehouse): Tables/<schema>/<table>\n",
    "table_specs_path = f\"{abfs}/Tables/ops/table_specs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "\n",
    "def _table_specs_schema() -> T.StructType:\n",
    "    return T.StructType(\n",
    "        [\n",
    "            T.StructField(\"source_layer\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"source_system\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"source_table_name\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"target_table_name\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"primary_keys_json\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"watermark_column\", T.StringType(), nullable=True),\n",
    "            T.StructField(\"max_watermark_cuttoff\", T.TimestampType(), nullable=True),\n",
    "            T.StructField(\"schema_json\", T.StringType(), nullable=True),\n",
    "            T.StructField(\"transform_mode\", T.StringType(), nullable=False),\n",
    "            T.StructField(\"transform_notebook\", T.StringType(), nullable=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def _parse_iso_utc_ts(value: str | None) -> datetime | None:\n",
    "    \"\"\"Parse ISO UTC timestamps like '2023-12-01T00:00:00Z' (or with fractional seconds).\n",
    "    Returns a naive datetime in UTC (Spark-friendly), or None if blank.\n",
    "    \"\"\"\n",
    "    s = (value or \"\").strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    # Accept trailing 'Z' shorthand for UTC\n",
    "    if s.endswith(\"Z\"):\n",
    "        s = s[:-1] + \"+00:00\"\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(s)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(\n",
    "            f\"Invalid ISO timestamp for max_watermark_cuttoff: {value!r}. \"\n",
    "            \"Expected e.g. '2023-12-01T00:00:00Z'\"\n",
    "        ) from e\n",
    "\n",
    "    if dt.tzinfo is None:\n",
    "        # Assume naive input is already UTC\n",
    "        return dt\n",
    "    return dt.astimezone(timezone.utc).replace(tzinfo=None)\n",
    "\n",
    "\n",
    "def _normalize_json_array_of_strings(value: str) -> str:\n",
    "    \"\"\"Validate and normalize PK JSON like '[\"id\", \"line_no\"]'.\"\"\"\n",
    "    s = (value or \"\").strip()\n",
    "    if not s:\n",
    "        raise ValueError(\"primary_keys_json is required (non-empty JSON array string).\")\n",
    "    try:\n",
    "        arr = json.loads(s)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"primary_keys_json is not valid JSON: {value!r}\") from e\n",
    "    if not isinstance(arr, list) or not arr or not all(isinstance(x, str) and x.strip() for x in arr):\n",
    "        raise ValueError(\n",
    "            \"primary_keys_json must be a non-empty JSON array of non-empty strings, \"\n",
    "            \"e.g. ['id'] or ['id','line_no']\"\n",
    "        )\n",
    "    return json.dumps([x.strip() for x in arr])\n",
    "\n",
    "\n",
    "def _req(name: str, value: str) -> str:\n",
    "    v = (value or \"\").strip()\n",
    "    if not v:\n",
    "        raise ValueError(f\"{name} is required\")\n",
    "    return v\n",
    "\n",
    "\n",
    "# Required values\n",
    "source_layer = _req(\"source_layer\", source_layer)\n",
    "source_system = _req(\"source_system\", source_system)\n",
    "source_table_name = _req(\"source_table_name\", source_table_name)\n",
    "target_table_name = _req(\"target_table_name\", target_table_name)\n",
    "primary_keys_json = _normalize_json_array_of_strings(primary_keys_json)\n",
    "\n",
    "# Optional values\n",
    "watermark_column = (watermark_column or \"\").strip() or None\n",
    "max_watermark_dt = _parse_iso_utc_ts(max_watermark_cuttoff)\n",
    "schema_json = (schema_json or \"\").strip() or None\n",
    "transform_mode = (transform_mode or \"\").strip().upper() or \"PASS\"\n",
    "transform_notebook = (transform_notebook or \"\").strip() or None\n",
    "\n",
    "valid_modes = {\"PASS\", \"FLATTEN\", \"CUSTOM\"}\n",
    "if transform_mode not in valid_modes:\n",
    "    raise ValueError(f\"transform_mode must be one of {sorted(valid_modes)}\")\n",
    "if transform_mode == \"CUSTOM\" and not transform_notebook:\n",
    "    raise ValueError(\"transform_notebook is required when transform_mode='CUSTOM'\")\n",
    "\n",
    "\n",
    "# Single-row source DF (the new/updated spec row)\n",
    "src_df = spark.createDataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"source_layer\": source_layer,\n",
    "            \"source_system\": source_system,\n",
    "            \"source_table_name\": source_table_name,\n",
    "            \"target_table_name\": target_table_name,\n",
    "            \"primary_keys_json\": primary_keys_json,\n",
    "            \"watermark_column\": watermark_column,\n",
    "            \"max_watermark_cuttoff\": max_watermark_dt,\n",
    "            \"schema_json\": schema_json,\n",
    "            \"transform_mode\": transform_mode,\n",
    "            \"transform_notebook\": transform_notebook,\n",
    "        }\n",
    "    ],\n",
    "    schema=_table_specs_schema(),\n",
    ")\n",
    "\n",
    "display(src_df)  # preview row to be merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "###### **Merge Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Target delta table at ABFSS path\n",
    "tgt = DeltaTable.forPath(spark, table_specs_path)\n",
    "\n",
    "merge_cond = (\n",
    "    \"t.source_layer = s.source_layer \"\n",
    "    \"AND t.source_system = s.source_system \"\n",
    "    \"AND t.source_table_name = s.source_table_name\"\n",
    ")\n",
    "\n",
    "(\n",
    "    tgt.alias(\"t\")\n",
    "       .merge(src_df.alias(\"s\"), merge_cond)\n",
    "       .whenMatchedUpdate(set={\n",
    "            \"target_table_name\": \"s.target_table_name\",\n",
    "            \"primary_keys_json\": \"s.primary_keys_json\",\n",
    "            \"watermark_column\": \"s.watermark_column\",\n",
    "            \"max_watermark_cuttoff\": \"s.max_watermark_cuttoff\",\n",
    "            \"schema_json\": \"s.schema_json\",\n",
    "            \"transform_mode\": \"s.transform_mode\",\n",
    "            \"transform_notebook\": \"s.transform_notebook\",\n",
    "            \"_updated_ts\": \"current_timestamp()\",\n",
    "        })\n",
    "       .whenNotMatchedInsert(values={\n",
    "            \"source_layer\": \"s.source_layer\",\n",
    "            \"source_system\": \"s.source_system\",\n",
    "            \"source_table_name\": \"s.source_table_name\",\n",
    "            \"target_table_name\": \"s.target_table_name\",\n",
    "            \"primary_keys_json\": \"s.primary_keys_json\",\n",
    "            \"watermark_column\": \"s.watermark_column\",\n",
    "            \"max_watermark_cuttoff\": \"s.max_watermark_cuttoff\",\n",
    "            \"schema_json\": \"s.schema_json\",\n",
    "            \"transform_mode\": \"s.transform_mode\",\n",
    "            \"transform_notebook\": \"s.transform_notebook\",\n",
    "            \"_created_ts\": \"current_timestamp()\",\n",
    "            \"_updated_ts\": \"current_timestamp()\",\n",
    "        })\n",
    "       .execute()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "###### **Preview Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "if not preview:\n",
    "    notebookutils.notebook.exit(\"\")\n",
    "\n",
    "df = spark.read.format(\"delta\").load(table_specs_path)\n",
    "display(df)\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}