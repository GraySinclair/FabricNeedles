{"cells":[{"cell_type":"markdown","source":["# Ops Deployment Accelerator\n","\n","<details><summary>Click to read instructions here...</summary>\n","\n","1. Click `Run all`\n","2. Set `Ops` as the `default lakehouse` in the left panel\n","3. Click `Run all` again\n","\n","</details>"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"25e63a80-d3ed-4610-a9d6-6b294ab5e625"},{"cell_type":"code","source":["# ---------------------- SAFE DEPLOYMENT ---------------------- #\n","# --- DEPLOY OPS OR RETURN OPS ARTIFACT IF EXISTS --- #\n","def idemp_deploy_lh(name: str, *, description: str = \"\", workspace_id: str, enable_schemas: bool = True):\n","    try: # Try to get lh artifact\n","        return notebookutils.lakehouse.get(name, workspace_id)\n","    except Exception:# If it doesnâ€™t exist (or lookup fails), create it\n","        print(\"Ops lakehouse not found, attempting to deploy...\", flush=True)\n","        definition = {\"enableSchemas\": True} if enable_schemas else None\n","        return notebookutils.lakehouse.create(name, description, definition, workspace_id)\n","\n","def raise_(value_name:str): \n","    raise ValueError(f\"{value_name} was not found\") # handle null variables\n","\n","# --- Fetch the current context --- #\n","ctx = notebookutils.runtime.context\n","\n","name_wsid = \"currentWorkspaceId\"\n","name_dlhid = \"defaultLakehouseId\"\n","\n","# get ws id from context\n","current_ws_id = ctx.get(name_wsid) or raise_(name_wsid)\n","\n","# fetch or create Ops\n","ops_lh = idemp_deploy_lh(\n","    name=\"Ops\",\n","    description=\"Operations / control lakehouse\",\n","    workspace_id=current_ws_id,\n","    enable_schemas=True\n",")\n","\n","\n","# Check ops lakehouse is set as default\n","default_lh_id = ctx.get(name_dlhid)\n","if default_lh_id!=ops_lh.id:\n","    notebookutils.notebook.exit(\n","        \"\\nYou should see this message exactly once! If you see it again-- its a dragon!\\n\"\n","        \"\\nStep 1: Set ops as the default lakehouse in the panel to the left! `Add Data Items` -> `From Onelake Catalog`\"\n","        \"\\nStep 2: Click `Run All` again\"\n","    )\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d35d0a7a-ed51-4d0f-a215-e693d3ae3678"},{"cell_type":"code","source":["# create schemas\n","for schema in (\"ops\", \"audit\"):\n","    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d3cfe9dc-a2c1-468c-91b5-9b79e811f370"},{"cell_type":"code","source":["# define fully qualified names\n","MANIFEST_FQN = f\"{ops_lh.displayName}.ops.ingestion_manifest\"\n","SPECTBL_FQN = f\"{ops_lh.displayName}.ops.specs\"\n","RESTCOL_FQN = f\"{ops_lh.displayName}.ops.rest_collection\"\n","NBLOG_FQN = f\"{ops_lh.displayName}.audit.notebook_run_log\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"87dd84dc-fbc8-4b41-b8b6-fc7d3fd419ab"},{"cell_type":"code","source":["spark.sql(f\"\"\"\n","CREATE TABLE IF NOT EXISTS {MANIFEST_FQN} (\n","  -- Identity\n","  manifest_id STRING NOT NULL COMMENT 'UUID for this ingestion job instance',\n","  table_name  STRING NOT NULL COMMENT 'Source table being ingested (e.g., invoice, staff)',\n","\n","  -- Ingestion Variables\n","  sink_path STRING NOT NULL COMMENT 'Sink path where the payload for this job was written',\n","  ingest_ts   TIMESTAMP NOT NULL DEFAULT current_timestamp() COMMENT 'UTC timestamp when the REST API pull completed and the data was written to sinkpath',\n","  status      STRING NOT NULL COMMENT 'Execution state of the ingestion job (NEW, RUNNING, DONE, FAILED)',\n","  running_ts  TIMESTAMP COMMENT 'UTC timestamp when a worker claimed this job for execution',\n","  done_ts     TIMESTAMP COMMENT 'UTC timestamp when ingestion and downstream processing completed',\n","  error       STRING COMMENT 'Error message when status = FAILED; NULL when status = DONE'\n",")\n","USING DELTA\n","TBLPROPERTIES (\n","  'delta.autoOptimize.optimizeWrite'  = 'true',\n","  'delta.autoOptimize.autoCompact'    = 'true',\n","  'delta.feature.allowColumnDefaults' = 'supported',\n","  'delta.columnMapping.mode'          = 'name',\n","  'delta.minReaderVersion'            = '2',\n","  'delta.minWriterVersion'            = '5'\n",");\n","\"\"\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"af7a289c-bf14-414b-942f-8033ebb0776b"},{"cell_type":"code","source":["spark.sql(f\"\"\"\n","CREATE TABLE IF NOT EXISTS {SPECTBL_FQN} (\n","  -- identity\n","  source_layer  STRING NOT NULL COMMENT '(e.g., bronze, silver, gold)',\n","  source_system STRING NOT NULL COMMENT 'Source system this table belongs to (e.g., lcvista, intacct) also used for schema',\n","  source_table_name STRING NOT NULL COMMENT 'Logical name (e.g., staff, accounts)',\n","  \n","  -- targets\n","  target_table_name STRING NOT NULL COMMENT 'Target sink (e.g., lcvista.staff)',\n","\n","  -- merge contract\n","  primary_keys_json STRING NOT NULL COMMENT 'JSON array of primary key columns for MERGE (e.g., [\"id\"] or [\"id\",\"line_no\"])',\n","  watermark_column STRING COMMENT 'Name of the source-system column used for incremental ingestion (e.g., modified, updated_at, last_changed); actual watermark values are tracked in ops.watermarks',\n","  max_watermark_cuttoff TIMESTAMP COMMENT 'Highest successfully processed watermark value; next run resumes strictly after this timestamp',\n","\n","  -- schema contract (Spark StructType JSON)\n","  schema_json STRING COMMENT 'Spark StructType serialized as JSON; enforced schema for reading source into sink',\n","\n","  -- controls / routing\n","  transform_mode STRING NOT NULL DEFAULT 'PASS' COMMENT 'Transformation mode: PASS, FLATTEN, or CUSTOM',\n","  transform_notebook STRING COMMENT 'Optional notebook name or path used when transform_mode = CUSTOM',\n","\n","  -- audit\n","  _created_ts TIMESTAMP NOT NULL DEFAULT current_timestamp() COMMENT 'UTC timestamp when this table spec was created',\n","  _updated_ts TIMESTAMP NOT NULL DEFAULT current_timestamp() COMMENT 'UTC timestamp when this table spec was last updated'\n",")\n","USING DELTA\n","TBLPROPERTIES (\n","  'delta.autoOptimize.optimizeWrite'  = 'true',\n","  'delta.autoOptimize.autoCompact'    = 'true',\n","  'delta.feature.allowColumnDefaults' = 'supported',\n","  'delta.columnMapping.mode'          = 'name',\n","  'delta.minReaderVersion'            = '2',\n","  'delta.minWriterVersion'            = '5'\n",")\n","\"\"\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b1bd5c22-d57f-459c-8c92-a743c51a5d90"},{"cell_type":"code","source":["spark.sql(f\"\"\"\n","CREATE TABLE IF NOT EXISTS {RESTCOL_FQN} (\n","  -- Identity\n","  source_system  STRING  NOT NULL COMMENT 'API system identifier (e.g., lcvista, intacct) used for routing API logic',\n","  table_name     STRING  NOT NULL COMMENT 'Logical target table name this endpoint populates in the Bronze/Silver pipeline',\n","\n","  -- Endpoint\n","  endpoint  STRING  NOT NULL COMMENT 'API endpoint to call (e.g., /v1/gl/accounts, /people)',\n","  http_method    STRING  NOT NULL COMMENT 'HTTP verb to use when calling the endpoint (typically GET or POST)',\n","  \n","  -- Execution\n","  watermark_column STRING COMMENT 'Name of the source-system column used for incremental ingestion (e.g., modified, updated_at, last_changed); actual watermark values are tracked in ops.watermarks',\n","  max_watermark_cuttoff TIMESTAMP COMMENT 'Highest successfully processed watermark value; next run resumes strictly after this timestamp',\n","  page_size      INT COMMENT 'Maximum number of records to request per API call; used for pagination and rate control',\n","  query_template STRING COMMENT 'Parameterized query string template injected at runtime (e.g., updatedAfter=watermark)',\n","  headers_json   STRING COMMENT 'Optional static HTTP headers serialized as JSON and merged into request headers'\n",")\n","USING DELTA\n","TBLPROPERTIES (\n","  'delta.autoOptimize.optimizeWrite'  = 'true',\n","  'delta.autoOptimize.autoCompact'    = 'true',\n","  'delta.feature.allowColumnDefaults' = 'supported',\n","  'delta.columnMapping.mode'          = 'name',\n","  'delta.minReaderVersion'            = '2',\n","  'delta.minWriterVersion'            = '5'\n",");\n","\"\"\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"76faa242-7230-4aa1-b6be-6600e2d3501c"},{"cell_type":"code","source":["spark.sql(f\"\"\"\n","CREATE TABLE IF NOT EXISTS {NBLOG_FQN} (\n","  -- Identity\n","  run_id          STRING    COMMENT 'Master pipeline id shared across all activities in the same scheduled run',\n","  notebook_name   STRING    COMMENT 'Name of the activity being logged',\n","  table_name      STRING    COMMENT 'Logical source table being ingested (one row per table per run)',\n","\n","  -- Log Details\n","  status          STRING    COMMENT 'Final execution status returned by run_notebook_with_retries: SUCCESS or FAILED',\n","  attempts        INT       COMMENT 'Number of notebook execution attempts including retries',\n","  elapsed_seconds DOUBLE    COMMENT 'Total wall-clock time spent running and retrying the notebook',\n","  error           STRING    COMMENT 'Final error message if the notebook failed; NULL when status = SUCCESS',\n","  log_ts          TIMESTAMP COMMENT 'Timestamp when this result row was written to the ops log table'\n",")\n","USING DELTA\n","TBLPROPERTIES (\n","  'delta.autoOptimize.optimizeWrite'  = 'true',\n","  'delta.autoOptimize.autoCompact'    = 'true',\n","  'delta.feature.allowColumnDefaults' = 'supported',\n","  'delta.columnMapping.mode'          = 'name',\n","  'delta.minReaderVersion'            = '2',\n","  'delta.minWriterVersion'            = '5'\n",")\n","\"\"\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fdde664f-4c21-4728-8821-7b3e6babaa3a"},{"cell_type":"code","source":["# debug outputs\n","if not ctx.get(\"isForPipeline\"):\n","    print(\"\\nLakehouse Artifact Details:\\n\", ops_lh.items(),flush=True)\n","    \n","    print(\"\\nContext Details:\\n\", ctx.items(),flush=True)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b8adacfa-580a-4983-b58b-40a5ad691063"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}